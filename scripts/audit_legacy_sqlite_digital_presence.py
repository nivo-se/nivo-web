#!/usr/bin/env python3
"""
Audit a legacy SQLite database for URLs, domains, emails, and contact info.
Read-only. Never modifies the SQLite DB.
Produces docs/LEGACY_SQLITE_DIGITAL_PRESENCE_AUDIT.md.

Usage:
    python3 scripts/audit_legacy_sqlite_digital_presence.py [--db path]
    python3 scripts/audit_legacy_sqlite_digital_presence.py --db data/nivo_optimized_original.db
"""
from __future__ import annotations

import argparse
import re
import sqlite3
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
DEFAULT_DB = REPO_ROOT / "data" / "legacy_nivo.sqlite"
OUTPUT_MD = REPO_ROOT / "docs" / "LEGACY_SQLITE_DIGITAL_PRESENCE_AUDIT.md"

# Heuristic: column names that likely contain digital presence data
CANDIDATE_PATTERNS = re.compile(
    r"url|website|homepage|domain|email|mail|contact|linkedin|facebook|instagram|"
    r"twitter|web_site|home_page|home_url|company_url|site_url",
    re.I,
)
# Tables we expect to potentially have orgnr / company keys
KEY_COLS = {"orgnr", "org_number", "company_id", "companyid", "id"}


def connect_readonly(db_path: Path) -> sqlite3.Connection:
    """Open SQLite in read-only immutable mode. Never modifies the DB."""
    uri = f"file:{db_path.resolve()}?mode=ro&immutable=1"
    return sqlite3.connect(uri, uri=True)


def get_tables(conn: sqlite3.Connection) -> list[tuple[str, str]]:
    """Return (name, sql) for each user table."""
    cur = conn.execute(
        "SELECT name, sql FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
    )
    return cur.fetchall()


def get_columns(conn: sqlite3.Connection, table: str) -> list[tuple[str, str]]:
    """Return (name, type) for each column."""
    cur = conn.execute(f"PRAGMA table_info([{table}])")
    return [(r[1], r[2]) for r in cur.fetchall()]


def is_candidate_column(name: str) -> bool:
    return bool(CANDIDATE_PATTERNS.search(name))


def count_rows(conn: sqlite3.Connection, table: str) -> int:
    cur = conn.execute(f"SELECT COUNT(*) FROM [{table}]")
    return cur.fetchone()[0]


def sample_distinct(conn: sqlite3.Connection, table: str, col: str, limit: int = 10) -> list[str]:
    """Sample up to N distinct non-null values (safe, read-only)."""
    cur = conn.execute(
        f"SELECT DISTINCT [{col}] FROM [{table}] WHERE [{col}] IS NOT NULL AND trim([{col}]) != '' LIMIT ?",
        (limit,),
    )
    return [str(r[0])[:200] for r in cur.fetchall()]


def count_non_null(conn: sqlite3.Connection, table: str, col: str) -> int:
    cur = conn.execute(
        f"SELECT COUNT(*) FROM [{table}] WHERE [{col}] IS NOT NULL AND trim([{col}]) != ''"
    )
    return cur.fetchone()[0]


def count_distinct_capped(conn: sqlite3.Connection, table: str, col: str, cap: int = 10000) -> int:
    cur = conn.execute(
        f"SELECT COUNT(DISTINCT [{col}]) FROM [{table}] WHERE [{col}] IS NOT NULL AND trim([{col}]) != ''"
    )
    n = cur.fetchone()[0]
    return min(n, cap) if cap else n


def has_orgnr(conn: sqlite3.Connection, table: str) -> bool:
    cols = [c[0].lower() for c in get_columns(conn, table)]
    return "orgnr" in cols or "org_number" in cols


def get_orgnr_column(table_cols: list[tuple[str, str]]) -> str | None:
    for name, _ in table_cols:
        if name.lower() in ("orgnr", "org_number"):
            return name
    return None


def coverage_by_orgnr(conn: sqlite3.Connection, table: str, url_col: str | None, email_col: str | None) -> dict:
    """Return counts: has_url, has_email, has_both."""
    orgnr_col = get_orgnr_column(get_columns(conn, table))
    if not orgnr_col:
        return {"total_orgnr": 0, "has_url": 0, "has_email": 0, "has_both": 0}
    url_cond = f"[{url_col}] IS NOT NULL AND trim([{url_col}]) != ''" if url_col else "1=0"
    email_cond = f"[{email_col}] IS NOT NULL AND trim([{email_col}]) != ''" if email_col else "1=0"
    sql = f"""
    SELECT
      COUNT(DISTINCT [{orgnr_col}]) as total,
      COUNT(DISTINCT CASE WHEN {url_cond} THEN [{orgnr_col}] END) as with_url,
      COUNT(DISTINCT CASE WHEN {email_cond} THEN [{orgnr_col}] END) as with_email,
      COUNT(DISTINCT CASE WHEN {url_cond} AND {email_cond} THEN [{orgnr_col}] END) as with_both
    FROM [{table}]
    WHERE [{orgnr_col}] IS NOT NULL AND trim([{orgnr_col}]) != ''
    """
    cur = conn.execute(sql)
    row = cur.fetchone()
    return {
        "total_orgnr": row[0],
        "has_url": row[1],
        "has_email": row[2],
        "has_both": row[3],
    }


def run_audit(db_path: Path) -> str:
    conn = connect_readonly(db_path)
    lines: list[str] = []
    lines.append("# Legacy SQLite Digital Presence Audit")
    lines.append("")
    lines.append("**Generated by:** `scripts/audit_legacy_sqlite_digital_presence.py`")
    lines.append("**Database:** " + str(db_path))
    lines.append("**Mode:** Read-only (no modifications to SQLite)")
    lines.append("")
    lines.append("---")
    lines.append("")

    tables = get_tables(conn)
    lines.append("## 1. Tables Overview")
    lines.append("")
    lines.append("| Table | Row Count |")
    lines.append("|-------|-----------|")

    table_info: dict[str, dict] = {}
    for name, _ in tables:
        n = count_rows(conn, name)
        lines.append(f"| {name} | {n:,} |")
        table_info[name] = {"rows": n, "cols": get_columns(conn, name)}

    lines.append("")
    lines.append("---")
    lines.append("")
    lines.append("## 2. Candidate Columns (URL, website, homepage, domain, email, contact, etc.)")
    lines.append("")

    all_candidates: list[tuple[str, str]] = []
    for table, info in table_info.items():
        for col, ctype in info["cols"]:
            if is_candidate_column(col):
                all_candidates.append((table, col))

    if not all_candidates:
        lines.append("No columns matched heuristic patterns. Listing all columns for manual review.")
        lines.append("")
        for table, info in table_info.items():
            lines.append(f"### {table}")
            for col, ctype in info["cols"]:
                lines.append(f"- `{col}` ({ctype})")
            lines.append("")
    else:
        for table, col in all_candidates:
            lines.append(f"### {table}.{col}")
            lines.append("")
            non_null = count_non_null(conn, table, col)
            distinct = count_distinct_capped(conn, table, col)
            samples = sample_distinct(conn, table, col, 10)
            lines.append(f"- **Non-null count:** {non_null:,}")
            lines.append(f"- **Distinct values (capped 10k):** {distinct:,}")
            lines.append("- **Sample values:**")
            for s in samples[:10]:
                esc = s.replace("|", "\\|")[:100]
                lines.append(f"  - `{esc}`")
            lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## 3. Coverage by orgnr")
    lines.append("")

    url_cols = [c for t, c in all_candidates if re.search(r"url|website|homepage|domain|web", c, re.I)]
    email_cols = [c for t, c in all_candidates if re.search(r"email|mail", c, re.I)]

    for table, info in table_info.items():
        orgnr_col = get_orgnr_column(info["cols"])
        if not orgnr_col:
            continue
        tbl_url = next((c for t, c in all_candidates if t == table and c in url_cols), None)
        tbl_email = next((c for t, c in all_candidates if t == table and c in email_cols), None)
        if not tbl_url and not tbl_email:
            continue
        cov = coverage_by_orgnr(conn, table, tbl_url or "", tbl_email)
        lines.append(f"### {table}")
        lines.append("")
        lines.append(f"- **Total orgnr (non-empty):** {cov['total_orgnr']:,}")
        lines.append(f"- **Orgnr with at least 1 URL:** {cov['has_url']:,}")
        lines.append(f"- **Orgnr with at least 1 email:** {cov['has_email']:,}")
        lines.append(f"- **Orgnr with both:** {cov['has_both']:,}")
        lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## 4. Join Keys")
    lines.append("")
    lines.append("| Table | Has orgnr? | orgnr column |")
    lines.append("|-------|------------|--------------|")
    for table, info in table_info.items():
        orgnr_col = get_orgnr_column(info["cols"])
        lines.append(f"| {table} | {'Yes' if orgnr_col else 'No'} | {orgnr_col or '—'} |")
    lines.append("")

    lines.append("---")
    lines.append("")
    lines.append("## 5. Proposed Mapping Plan")
    lines.append("")
    lines.append("(To be refined after reviewing samples above.)")
    lines.append("")
    lines.append("| Source (SQLite) | Target (Postgres) | Normalization |")
    lines.append("|-----------------|-------------------|---------------|")
    lines.append("| homepage / website / url | companies.homepage | scheme https, trim, reject junk |")
    lines.append("| email / mail | companies.email | lowercase, basic validation, dedupe |")
    lines.append("")
    lines.append("**Conflict rules:**")
    lines.append("- Only fill Postgres `homepage` / `email` when currently NULL or empty.")
    lines.append("- Prefer homepage/website over social URLs for `homepage`.")
    lines.append("")

    conn.close()
    return "\n".join(lines)


def main() -> int:
    ap = argparse.ArgumentParser(description="Audit legacy SQLite for digital presence data (read-only)")
    ap.add_argument(
        "--db",
        type=Path,
        default=DEFAULT_DB,
        help=f"Path to SQLite DB (default: {DEFAULT_DB})",
    )
    ap.add_argument(
        "--out",
        type=Path,
        default=OUTPUT_MD,
        help=f"Output markdown path (default: {OUTPUT_MD})",
    )
    args = ap.parse_args()
    db = args.db if args.db.is_absolute() else (REPO_ROOT / args.db)
    if not db.exists():
        print(f"❌ Database not found: {db}")
        return 1
    print(f"Auditing (read-only): {db}")
    md = run_audit(db)
    args.out.parent.mkdir(parents=True, exist_ok=True)
    args.out.write_text(md, encoding="utf-8")
    print(f"✅ Wrote {args.out}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
